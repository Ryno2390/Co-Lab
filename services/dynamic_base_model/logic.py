from typing import Optional, Any
from openai import AsyncOpenAI # Example using OpenAI

# Import settings, potentially shared or service-specific
try:
    from ...config import settings
    llm_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
    # Choose a model suitable for general instruction following
    # Could be the same as decomposition or a slightly more capable one
    DYNAMIC_MODEL = settings.DECOMPOSITION_MODEL # Example: Reuse decomposition model initially
    # Or: DYNAMIC_MODEL = "gpt-4o-mini"
except ImportError:
     print("Warning: Could not import main settings. Using fallback LLM client init.")
     import os
     llm_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
     DYNAMIC_MODEL = "gpt-3.5-turbo" # Fallback model
except Exception as e:
    print(f"Error initializing LLM client for dynamic base model: {e}")
    llm_client = None


async def run_dynamic_prompt(prompt: str) -> Optional[Any]:
    """
    Executes a dynamically constructed prompt using a general-purpose base LLM.

    Args:
        prompt: The complete prompt string including instructions and context.

    Returns:
        The raw content generated by the LLM (could be text, JSON string, etc.),
        or None if an error occurs.
    """
    if not llm_client:
        print("Error: LLM client not initialized for dynamic base model.")
        raise ConnectionError("LLM client not available for dynamic base model.")

    print(f"Running dynamic prompt (first 100 chars): '{prompt[:100]}...'")

    # Construct messages for chat models
    # Assuming the prompt contains all necessary instructions and context
    # A system prompt might still be useful depending on the base model
    messages = [
        # Optional: Add a system prompt if needed for the base model's behavior
        # {"role": "system", "content": "You are a helpful AI assistant."},
        {"role": "user", "content": prompt}
    ]

    try:
        print(f"Sending prompt to Dynamic Base Model LLM ({DYNAMIC_MODEL})...")
        response = await llm_client.chat.completions.create(
            model=DYNAMIC_MODEL,
            messages=messages,
            temperature=0.7, # Allow more creativity than decomposition/synthesis maybe
            # max_tokens might be relevant depending on expected output
        )
        generated_content = response.choices[0].message.content
        print("Successfully received response from Dynamic Base Model LLM.")
        # Return the raw content; pre-processing/parsing might happen in the Core AI Synthesizer
        return generated_content.strip() if generated_content else None

    except Exception as e:
        print(f"Error calling LLM API for dynamic base model: {e}")
        raise Exception(f"LLM API call failed for dynamic base model: {e}")